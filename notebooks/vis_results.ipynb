{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results visualisation notebook\n",
    "\n",
    "Use this notebook to visualise results generated from `scripts/run_eval.py`. Set the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILE = 'M:/models/deepinv-experiments/eval_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from py_markdown_table.markdown_table import markdown_table\n",
    "from itertools import zip_longest\n",
    "\n",
    "def isnotNone(x): return x is not None and x != \"\"\n",
    "def filt(lbl): [results[id].get(lbl, \"\") for id in results.keys()]\n",
    "\n",
    "with open(RESULTS_FILE, \"r\") as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, metrics, metrics_init = filt('title'), filt('metrics'), filt('metrics_init')\n",
    "\n",
    "titles = [\"No learning\"] + titles\n",
    "metrics = [metrics_init[0]] + metrics #0 here arbitrary as all the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\n",
    "    \"Method\": title,\n",
    "    \"metric1\": round(metric[0], 2) if isnotNone(metric[0]) else \"\",\n",
    "    \"metric2\": round(metric[1], 2) if isnotNone(metric[1]) else \"\",\n",
    "} for title, metric in zip_longest(titles, metrics)]\n",
    "\n",
    "print(markdown_table(data).set_params(row_sep=\"markdown\").get_markdown())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
